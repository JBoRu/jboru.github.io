<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jinhao Jiang</title>
    <meta content="Jinhao Jiang, https://jboru.github.io" name="keywords">

    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: inherit;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 14px;
            background: #eee;
        }

        h2 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: left;
            width: 200px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }

        /* Language switcher styles */
        .lang-switch {
            position: absolute;
            top: 10px;
            right: 10px;
            padding: 5px 10px;
            background: #fff;
            border: 1px solid #ddd;
            border-radius: 3px;
            cursor: pointer;
            font-weight: bold;
        }
        
        .lang-switch:hover {
            background: #f0f0f0;
        }
        
        /* Content language classes */
        .en {
            display: block;
        }
        
        .zh {
            display: none;
        }
    </style>


    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'UA-164510176-1');
    </script>

</head>


<body>

    <!--photo and basic information-->
    <div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 150px;">
        <!-- Language switch button -->
        <button id="lang-switch" class="lang-switch" onclick="toggleLanguage()">中文</button>
        <div style="margin: 0px auto; width: 100%;">
            <img title="jinhao" style="float: left; padding-left: .01em; height: 120px;"
                 src="images/jiangjinhao.jpg">
            <div style="padding-left: 12em; vertical-align: top; height: 150px;">
                <span style="line-height: 150%; font-size: 20pt;" class="en">Jinhao Jiang (蒋锦昊)</span>
                <span style="line-height: 150%; font-size: 20pt;" class="zh">蒋锦昊 (Jinhao Jiang)</span><br>
                <span class="en"><a href="http://ai.ruc.edu.cn/">Gaoling School of Artificial Intelligence</a>, <a
                        href="https://en.ruc.edu.cn/">Renmin University of China(RUC)</a></span>
                <span class="zh"><a href="http://ai.ruc.edu.cn/">中国人民大学</a>, <a
                        href="https://en.ruc.edu.cn/">高瓴人工智能学院</a></span><br>
                <span class="en"><strong>Address</strong>: No.59 Zhongguancun Street, Haidian District Beijing, 100872, P.R. China</span>
                <span class="zh"><strong>地址</strong>: 中国北京市海淀区中关村大街59号，100872</span><br>
                <span class="en"><strong>Email</strong>: jiangjinhao [at] ruc.edu.cn</span>
                <span class="zh"><strong>邮箱</strong>: jiangjinhao [at] ruc.edu.cn</span>
            </div>
        </div>
    </div>
    <!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

    
    <!--biography-->
    <div style="clear: both;">
        <div class="section">
            <h2 class="en">About Me <a href="https://github.com/JBoRu">[GitHub]</a>
                <a href="https://scholar.google.com/citations?user=TeFKijMAAAAJ&hl=en">[Google Scholar]</a>
            </h2>
            <h2 class="zh">关于我 <a href="https://github.com/JBoRu">[GitHub]</a>
                <a href="https://scholar.google.com/citations?user=TeFKijMAAAAJ&hl=en">[Google Scholar]</a>
            </h2>
            <div class="paper">
                <p class="en">I am a fourth-year Ph.D. student (expected to graduate in June 2026) supervised by <a href="http://playbigdata.ruc.edu.cn/batmanfly/">Prof. Xin Zhao</a> from <a href="http://ai.ruc.edu.cn/english/index.htm">GSAI</a>, <a href="https://ruc.edu.cn">Renmin University of China</a>. Prior to this, I obtained a bachelor's degree from the <a href="https://www.uestc.edu.cn/">University of Electronic Science and Technology of China</a> in July 2021. I have a broad interest in Natural Language Processing, Large Language Model, and Agent.</p>
                
                <p class="zh">我是中国人民大学高瓴人工智能学院四年级博士生（预计2026年6月毕业），导师为<a href="http://playbigdata.ruc.edu.cn/batmanfly/">赵鑫教授</a>。此前，我于2021年7月获得<a href="https://www.uestc.edu.cn/">电子科技大学</a>学士学位。我对自然语言处理、大语言模型和智能体有广泛的研究兴趣。</p>
            </div>
        </div>
    </div>

    <!--Research Interest-->
    <div style="clear: both;">
        <div class="section clearfix">
            <h2 class="en">Research Interest</h2>
            <h2 class="zh">研究兴趣</h2>
            <div class="paper">
                <p class="en">My research interest focuses on <strong>LLM</strong> and <strong>Agent</strong>, with an emphasis on
                <strong>fundamental capabilities (world knowledge & complex reasoning) of LLM</strong> and
                <strong>agent applications</strong>, specifically:</p>
                
                <p class="zh">我的研究兴趣集中在<strong>大语言模型（LLM）</strong>和<strong>智能体（Agent）</strong>，特别关注
                <strong>大语言模型的基础能力（世界知识和复杂推理）</strong>以及
                <strong>智能体应用</strong>，具体包括：</p><br>
            
                <ul class="en">
                    <li>Enhancing internal reasoning capabilities: Through continue pre-training (CPT), supervised fine-tuning
                        (SFT), and reinforcement learning (RL) training, expand the knowledge boundaries of LLMs and enhance
                        the inherent general reasoning abilities of LLMs (such as encyclopedic knowledge, mathematics, and
                        code).
                    </li><br>
                    <li>Enhancing the ability to call external tools: Improve the ability of LLMs to call external tools (such
                        as code, calculators, and search engines).
                    </li><br>
                    <li>Agentic LLM: Enhance the application of LLM-based agent in various scenarios,
                        such as complex structured data (such as knowledge graphs, databases, Excel spreadsheets, and tables),
                        general retrieval scenarios (such as AI Searcher, Deep Research), etc.
                    </li><br>
                </ul>
                
                <ul class="zh">
                    <li>增强内部推理能力：通过持续预训练（CPT）、监督微调（SFT）和强化学习（RL）训练，拓展大语言模型的知识边界，提升大语言模型固有的通用推理能力（如百科知识、数学和代码）。
                    </li><br>
                    <li>增强调用外部工具的能力：提高大语言模型调用外部工具（如代码、计算器和搜索引擎）的能力。
                    </li><br>
                    <li>通用智能体应用：增强基于大语言模型的智能体在各类场景中的应用，如复杂结构化数据（如知识图谱、数据库、Excel电子表格和表格），通用检索场景（如AI搜索器、深度研究）等。
                    </li><br>
                </ul>
                
                <alert class="en">
                    I am currently seeking job opportunities in both academic and industry. I am expected to graduate in July
                    2026. If you are interested in me, please do not hesitate to contact me via Email.
                </alert>
                
                <alert class="zh">
                    我目前正在寻找学术界和工业界的工作机会。我预计将于2026年7月毕业。如果您对我感兴趣，请通过邮件联系我。
                </alert>
            </div>
        </div>
    </div>

    <!--News-->
    <div style="clear: both;">
        <div class="section">
            <h2 id="news" class="en">News</h2>
            <h2 id="news" class="zh">新闻</h2><br>
                <ul class="en">
                    <li>[2025-08-21] I have five papers accepted by EMNLP 2025, including <a href="https://arxiv.org/abs/2505.10063">CAFE</a> (Main), <a>StickerTTS</a> (Main), <a href="https://arxiv.org/pdf/2505.18105">ManuSearch</a> (Findings), <a href="https://arxiv.org/pdf/2503.05592">R1-Searcher++</a> (Findings), and <a href="https://arxiv.org/pdf/2505.16834?">SimpleDeepSearcher</a> (Findings), congratulations to my co-authors!
                    <li>[2025-05-23] We release <a href="https://arxiv.org/pdf/2505.18105">ManuSearch</a>, which aims to push the LLM-based AI search with a curated complex search benchmark and a strong training-free multi-agent framework.
                    <li>[2025-05-22] We release <a href="https://arxiv.org/pdf/2505.16834?">SimpleDeepSearcher</a> and <a href="https://arxiv.org/pdf/2505.17005">R1-Searcher++</a>, which are the data engineering method for collecting small but refined SFT dataa, and a novel RL framework for incentivizing the dynamic knowledge acquisition of LLMs from internal knowledge and external tools.
                    <li>[2025-05-21] I have four papers accepted by ACL 2025 main conference, including <a href="https://arxiv.org/pdf/2412.17743">YuLan-Mini</a>, <a href="https://arxiv.org/pdf/2407.18743">Llama-3-SynE</a>, <a href="https://arxiv.org/pdf/2402.11163">KG-Agent</a>, and <a href="https://arxiv.org/pdf/2502.07365">LongReD</a>, congratulations to my co-authors!
                    <li>[2025-04-29] We release <a href="https://arxiv.org/abs/2504.20426v1">RV-Syn</a>, which is a rational and verifiable mathematical data synthesis method based on structured function library. It has achieved more efficent scaling curve compared to latest methodes, such as ScaleQuest, NuminaMath.
                    <li>[2025-04-22] I'll present my work <a href="https://openreview.net/pdf?id=h1XoHOd19I">"Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment"</a> poster at ICLR-2025 in Singapore. It's on April 26th, from 15:00 - 17:30 in Hall 3 + Hall 2B. Welcome to have a chat!
                    <li>[2025-03-07] We release <a href="https://arxiv.org/pdf/2503.05592">R1-Searcher</a>, which is the first technical report to apply the RL of the R1 paradigm to the RAG scenario. It has achieved significant performance improvements across multiple evaluation datasets, marking an important step towards Deep Research!
                </ul>
                
                <ul class="zh">
                    <li>[2025-08-21] 我有五篇论文被EMNLP 2025主会议接收，包括 <a href="https://arxiv.org/abs/2505.10063">CAFE</a> (Main), <a>StickerTTS</a> (Main), <a href="https://arxiv.org/pdf/2505.18105">ManuSearch</a> (Findings), <a href="https://arxiv.org/pdf/2503.05592">R1-Searcher++</a> (Findings), and <a href="https://arxiv.org/pdf/2505.16834?">SimpleDeepSearcher</a> (Findings), 祝贺我的合作者！
                    <li>[2025-05-23] 我们发布了 <a href="https://arxiv.org/pdf/2505.18105">ManuSearch</a>，旨在推动基于大语言模型的AI搜索，并提供了一个精心策划的复杂搜索基准和一个强大的通用免训的多智能体框架。
                    <li>[2025-05-22] 我们发布了 <a href="https://arxiv.org/pdf/2505.16834?">SimpleDeepSearcher</a> 和 <a href="https://arxiv.org/pdf/2505.17005">R1-Searcher++</a>，它们是收集少而精炼的SFT数据和激励LLM从内部知识和外部工具中动态获取知识的新方法。
                    <li>[2025-05-21] 我有四篇论文被ACL 2025主会议接收，包括 <a href="https://arxiv.org/pdf/2412.17743">YuLan-Mini</a>、<a href="https://arxiv.org/pdf/2407.18743">Llama-3-SynE</a>、<a href="https://arxiv.org/pdf/2402.11163">KG-Agent</a>和<a href="https://arxiv.org/pdf/2502.07365">LongReD</a>，祝贺我的合作者！
                    <li>[2025-04-29] 我们发布了 <a href="https://arxiv.org/abs/2504.20426v1">RV-Syn</a>，它是一种基于结构化函数库的理性且可验证的数学数据合成方法。与最新的方法相比，它具有更高效的缩放曲线，如ScaleQuest和NuminaMath。
                    <li>[2025-04-22] 我将在新加坡的ICLR-2025会议上展示我的工作 <a href="https://openreview.net/pdf?id=h1XoHOd19I">"Mix-CPT: 通过解耦知识学习和格式对齐的领域适应框架"</a>。展示时间是4月26日下午15:00-17:30，地点在3号厅+2B厅。欢迎来交流！
                    <li>[2025-03-07] 我们发布了 <a href="https://arxiv.org/pdf/2503.05592">R1-Searcher</a>，这是首个将R1范式的强化学习应用于RAG场景的技术报告。它在多个评估数据集上取得了显著的性能提升，标志着向深度研究迈出了重要一步！
                </ul>

            <!-- <details>
                <summary><span style="color:blue">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2024]</span></summary>
                <ul>
                    <li>2024-12: One paper about LLM evaluation was accepted by <a href="https://aaai.org/conference/aaai/aaai-25/">AAAI 2025</a>; We won the First Price in <a href="https://mp.weixin.qq.com/s/tANdhJhfKtO-ZZtVI2wM6A">Financial GraphRAG Competition</a>.</li>
                    <li>2024-02: One paper about text simplification was accepted by <a href="https://lrec-coling-2024.org/about-lrec-coling/">COLING 2024</a>.</li>                
                </ul>
            </details> -->

            <!-- <details>
                <summary><span style="color:blue">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2023]</span></summary>
                <ul><br>-->
                    <!-- <li>2023-12: We won the Best Innovation Award in <a href="https://mp.weixin.qq.com/s/sfJPI5yIIE9igTZlpbU5QQ">Yangtze River Delta Fintech Innovation & Application Global Competition</a>.</li> -->
                <!-- </ul> -->
            <!-- </details> -->
        </div>
    </div>

    <!-- Research Highlights
    <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers">Research Highlights (<a href="https://scholar.google.com/citations?user=Q0F92XIAAAAJ&hl=en">[Full List]</a>)</h2>


            <h4><alert>Question Answering</alert></h4>
            <div class="paper"><img class="paper" src="./resources/paper_icon/naacl_demo_short.gif"
                title="">
                <div>We have developed question answering systems under different scenarios. 
                For Machine Reading Comprehension (MRC), we proposed different methods to improve the accuracy of answering questions over passages [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6458/6314">AAAI'20</a>; <a href="https://aclanthology.org/2023.findings-acl.391.pdf">ACL'23</a>]. 
                For Knowledge Graph Question Answering (KGQA), we investegated and improved different modules in the QA framework, which achieve SOTA results in a variety of scenarios (e.g., simple questions [<a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=5443&context=sis_research">IJCAI'19</a>], complex questions [<a href="https://arxiv.org/pdf/2101.03737">WSDM'20</a>; <a href="https://aclanthology.org/2020.acl-main.91.pdf">ACL'20</a>; <a href="https://arxiv.org/pdf/2306.06872">ACL'23</a>], conversational questions [<a href="https://aclanthology.org/2021.acl-long.255.pdf">ACL'21</a>]).
                For Question Answering with Databases, we explored to generate complex questions in a specific domain [<a href="https://arxiv.org/pdf/2310.08395">EMNLP'23</a>; <a href="https://arxiv.org/abs/2402.13125">AAAI'25</a>] and align Large Language Models (LLMs) to a domain-specific database [<a href="https://arxiv.org/pdf/2402.16567">CIKM'24</a>].
                For Multi-modal Question Answering, we discussed the applications of LLMs and its safety issue [<a href="http://arxiv.org/abs/2311.09050.pdf">MM'23</a>; <a href="https://arxiv.org/pdf/2402.00357">IJCAI'24</a>;<a href="https://arxiv.org/pdf/2311.17600">ECCV'24</a>].<br>
                
                There are surveys [<a href="https://arxiv.org/pdf/2108.06688">TKDE'22</a>] you can start with.
                We have demonstration pages [<a href="http://49.52.27.116:8000/test">Demonstration Page</a> (only works within ECNU)] that you can try on!
                </div>
                <div class="spanner">
                </div>
            </div> 

            <h4><alert>Educational NLP</alert></h4>
            <div class="paper"><img class="paper" src="./resources/paper_icon/viscgec.gif"
                title="">
                <div>We have worked on the educational NLP tasks, such as Math Word Problem (MWP) solving [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/21723">AAAI demo'22</a>; <a href="https://arxiv.org/pdf/2305.04091">ACL'23</a>; <a href="https://arxiv.org/pdf/2310.16535">EMNLP Finding'23</a>], Chinese Spelling Check (CSC) [<a href="https://link.springer.com/chapter/10.1007/978-3-031-44699-3_29">NLPCC'23</a>], Grammatical Error Correction (GEC) [<a href="https://arxiv.org/pdf/2311.04906">CIKM'23</a>;<a href="https://arxiv.org/pdf/2311.04906">COLING'25</a>;<a href="https://arxiv.org/pdf/2311.04906">NAACL'25</a>], Text Simplification [<a href="https://arxiv.org/abs/2402.14704">COLING'24</a>], and Essay Scoring [<a href="https://kclpure.kcl.ac.uk/ws/portalfiles/portal/266055350/AIED_2024_Chen_et_al.pdf">AIED'24</a>;<a href="https://arxiv.org/pdf/2407.12857">EMNLP Finding'24</a>].<br>
                
                There are surveys [<a href="https://arxiv.org/pdf/2401.07518">ArXiv</a>] you can start with.
                </div>
                <div class="spanner">
                </div>
            </div>            
        </div>
    </div>
    <br> -->

    <div style="clear: both;">
        <div class="section clearfix">
            <h2 class="en">Experience</h2>
            <h2 class="zh">工作经历</h2><br>
            <ul class="en">
                <li>2025/01 - present: ByteDance-Seed (字节跳动-Seed) LLM Research Intern
                </li>
                <li>2024/10 - 2024/12: BAAI (智源研究院) LLM Research Intern
                </li>
                <li>2023/07 - 2024/10: Boss Zhipin (BOSS直聘) LLM Research Intern
                </li>
            </ul>
            <ul class="zh">
                <li>2025/01 - 至今: 字节跳动-Seed 大语言模型研究实习生
                </li>
                <li>2024/10 - 2024/12: 智源研究院 大语言模型研究实习生
                </li>
                <li>2023/07 - 2024/10: BOSS直聘 大语言模型研究实习生
                </li>
            </ul>
        </div>
    </div>

    
    <!--Research Highlights-->
    <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers" class="en">Selected Publications</h2>
            <h2 id="confpapers" class="zh">精选论文</h2>
            <div class="en">(* indicates equal contribution, &dagger; indicates corresponding author)</div>
            <div class="zh">(* 表示共同一作, &dagger; 表示通讯作者)</div><br><br>
            <div class="en">Here, I've listed my work as a (co-) first author. For the complete list of my publications, please visit my <a href="https://scholar.google.com/citations?user=TeFKijMAAAAJ&hl=en">Google Scholar</a> profile.</div>
            <div class="zh">这里列出了我作为（共同）第一作者的工作。有关我发表的完整论文列表，请访问我的<a href="https://scholar.google.com/citations?user=TeFKijMAAAAJ&hl=en">谷歌学术</a>主页。</div><br><br>

            <div class="paper">
            <ul>
                <li>
                    <a href="https://arxiv.org/pdf/2505.18105.pdf">ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework</a><br><br>
                    Lisheng Huang*, Yichen Liu*, <strong>Jinhao Jiang</strong>*, Rongxiang Zhang, Jiahao Yan, Junyi Li, Wayne Xin Zhao<br><br>
                    <i>EMNLP-Findings, 2025</i> <br>
                    <br>
                    </li>

                <li>
                    <a href="https://arxiv.org/pdf/2503.05592.pdf">R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning</a><br><br>
                    Huatong Song*, <strong>Jinhao Jiang</strong>*, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian Min, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen<br><br>
                    <i>EMNLP-Findings, 2025</i> <br>
                    <br>
                    </li>

                <li>
                <a href="https://arxiv.org/pdf/2503.05592.pdf">R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning</a><br><br>
                Huatong Song*, <strong>Jinhao Jiang</strong>*, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao&dagger;, Lei Fang, Ji-Rong Wen<br><br>
                <i>Technical Report, 2025</i> <br>
                <br>
                </li>
            </ul>
            
            <ul>
                <li>
                <a href="https://arxiv.org/abs/2504.20426v1">RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function library</a><br><br>
                Jiangpeng Wang*, <strong>Jinhao Jiang</strong>*, Zhiqiang Zhang, Jun Zhou, Wayne Xin Zhao&dagger;<br><br>
                <i>arXiv, 2025</i> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/abs/2505.10063">CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-document Question Answering Capability</a><br><br>
                Han Peng*, <strong>Jinhao Jiang</strong>*, Zican Dong*, Wayne Xin Zhao&dagger;, Lei Fang<br><br>
                <i>EMNLP, 2025</i> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2412.09413?.pdf">Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems</a><br><br>
                Yingqian Min*, Zhipeng Chen*, <strong>Jinhao Jiang</strong>*, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao&dagger;, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen<br><br>
                <i>Technical Report, 2025</i> <br>
                <br>
                </li>
            </ul>


            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2411.11694.pdf">Enhancing LLM Reasoning with Reward-guided Tree Search</a><br><br>
                <strong>Jinhao Jiang</strong>*, Zhipeng Chen*, Yingqian Min*, Jie Chen, Xiaoxue Cheng, Jiapeng Wang, Yiru Tang, Haoxiang Sun, Jia Deng, Wayne Xin Zhao&dagger;, Zheng Liu, Dong Yan, Jian Xie, Zhongyuan Wang, Ji-Rong Wen<br><br>
                <i>Technical Report, 2024</i> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2412.09413?.pdf">Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment</a><br><br>
                <strong>Jinhao Jiang</strong>*, Junyi Li*, Wayne Xin Zhao&dagger;, Yang Song, Tao Zhang, Ji-Rong Wen<br><br>
                <i>International Conference on Learning Representations (ICLR), 2025</i> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2412.12881.pdf">RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement</a><br><br>
                <strong>Jinhao Jiang</strong>*, Jiayi Chen*, Junyi Li*, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao&dagger;, Yang Song, Tao Zhang<br><br>
                <i>The North American Chapter of the Association for Computational Linguistics (NAACL), 2025</i> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2402.11163.pdf">KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph</a><br><br>
                <strong>Jinhao Jiang</strong>*, Kun Zhou*, Wayne Xin Zhao&dagger;, Yang Song, Chen Zhu, Hengshu Zhu, Ji-Rong Wen<br><br>
                <i>The 63rd Annual Meeting of the Association for Computational Linguistics (ACL), 2025</i> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2305.09645.pdf">StructGPT: A General Framework for Large Language Model to Reason over Structured Data</a><br><br>
                <strong>Jinhao Jiang</strong>*, Kun Zhou*, Zican Dong, Keming Ye, Wayne Xin Zhao&dagger;, Ji-Rong Wen<br><br>
                <i>The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023</i> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2305.09645.pdf">ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph</a><br><br>
                <strong>Jinhao Jiang</strong>, Kun Zhou, Wayne Xin Zhao&dagger;, Yaliang Li, Ji-Rong Wen<br><br>
                <i>The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023</i> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/abs/2212.00959">UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph</a><br><br>
                <strong>Jinhao Jiang</strong>*, Kun Zhou*, Wayne Xin Zhao&dagger;, Ji-Rong Wen<br><br>
                <i>International Conference on Learning Representations (ICLR), 2023</i><br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2205.01841.pdf">Great Truths are Always Simple: A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models</a><br><br>
                <strong>Jinhao Jiang</strong>*, Kun Zhou*, Wayne Xin Zhao&dagger;, Ji-Rong Wen<br><br>
                <i>The North American Chapter of the Association for Computational Linguistics (NAACL-Findings), 2022</i><br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2108.06688.pdf">Complex Knowledge Base Question Answering: A Survey</a><br><br>
                Yunshi Lan*, Gaole He*, <strong>Jinhao Jiang</strong>, Jing Jiang, Wayne Xin Zhao&dagger;, Ji-Rong Wen<br><br>
                <i>IEEE Transactions on Knowledge and Data Engineering (TKDE), 2022</i> <br>
                <br>
                </li>
            </ul>

        </div>
    </div>
    <br>

    <!--Grants-->
    <div style="clear: both;">
        <div class="section">
            <h2 class="en">Grants</h2>
            <h2 class="zh">奖项</h2>
            <div class="paper">
                <ul class="en">
                    <li>2021 Outstanding Graduates of Sichuan Province (winning ratio 3.7%), Education Department of Sichuan.</li>
                    <li>2020 China National Scholarship (top 1.5%), Ministry of Education of the People's Republic of China.</li>
                    <li>2019 China National Scholarship (top 1.5%), Ministry of Education of the People's Republic of China.</li>
                </ul>
                <ul class="zh">
                    <li>2021 四川省优秀毕业生（获奖比例3.7%），四川省教育厅。</li>
                    <li>2020 国家奖学金（前1.5%），中华人民共和国教育部。</li>
                    <li>2019 国家奖学金（前1.5%），中华人民共和国教育部。</li>
                </ul>
                <div class="spanner"></div>
            </div>
        </div>
    </div>


    <!--Professional Service-->
    <div style="clear: both;">
        <div class="section">
            <h2 class="en">Professional Service</h2>
            <h2 class="zh">学术服务</h2>
            <div class="paper">
                <ul class="en">
                    <li>Journal: TALLIP, Computational Intelligence, Information Retrieval Journa</li>
                    <li>Conference: ICLR, NIPS, ACL, EMNLP</li>
                </ul>
                <ul class="zh">
                    <li>期刊: TALLIP, Computational Intelligence, Information Retrieval Journal</li>
                    <li>会议: ICLR, NIPS, ACL, EMNLP</li>
                </ul>
                <div class="spanner"></div>
            </div>
        </div>
    </div>
</body>

<script>
    // Language toggle function
    function toggleLanguage() {
        // Get all elements with language classes
        const enElements = document.querySelectorAll('.en');
        const zhElements = document.querySelectorAll('.zh');
        const langSwitch = document.getElementById('lang-switch');
        
        // Check current language and switch
        const isCurrentlyEnglish = langSwitch.textContent === '中文';
        
        // Set visibility based on selected language
        enElements.forEach(el => {
            el.style.display = isCurrentlyEnglish ? 'none' : 'block';
        });
        
        zhElements.forEach(el => {
            el.style.display = isCurrentlyEnglish ? 'block' : 'none';
        });
        
        // Toggle button text
        langSwitch.textContent = isCurrentlyEnglish ? 'English' : '中文';
        
        // Remember language preference
        localStorage.setItem('language', isCurrentlyEnglish ? 'zh' : 'en');
        
        // Log for debugging
        console.log('Language toggled to: ' + (isCurrentlyEnglish ? 'Chinese' : 'English'));
        return false; // Prevent default action
    }
    
    // Initialize language display on page load
    document.addEventListener('DOMContentLoaded', function() {
        // Check for saved language preference
        const savedLanguage = localStorage.getItem('language');
        
        if (savedLanguage === 'zh') {
            // Simulate a click on the language button to switch to Chinese
            document.getElementById('lang-switch').click();
        }
    });
</script>

</html>